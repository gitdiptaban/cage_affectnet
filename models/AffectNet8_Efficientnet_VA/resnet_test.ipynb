{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the annotations for training and validation from separate CSV files\n",
    "IMAGE_FOLDER = r\"D:\\\\DebosmitaPhD\\\\VALENCE-AROUSAL\\\\AffectNet\\\\train_set\\\\images\"\n",
    "IMAGE_FOLDER_TEST = r\"D:\\\\DebosmitaPhD\\\\VALENCE-AROUSAL\\\\AffectNet\\\\val_set\\\\images\"\n",
    "train_annotations_path = (\n",
    "    r\"D:\\\\DebosmitaPhD\\\\VALENCE-AROUSAL\\\\CAGE-Affectnet-CVPR\\\\affectnet_annotations\\\\train_set_annotation_without_lnd.csv\"\n",
    ")\n",
    "valid_annotations_path = (\n",
    "    r\"D:\\\\DebosmitaPhD\\\\VALENCE-AROUSAL\\\\CAGE-Affectnet-CVPR\\\\affectnet_annotations\\\\val_set_annotation_without_lnd.csv\"\n",
    ")\n",
    "train_annotations_df = pd.read_csv(train_annotations_path)\n",
    "valid_annotations_df = pd.read_csv(valid_annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image folder exists: True\n",
      "Sample files in the directory: ['0.jpg', '1.jpg', '10.jpg', '100.jpg', '100000.jpg']\n",
      "First image exists: True\n"
     ]
    }
   ],
   "source": [
    "# Check if the folder exists\n",
    "print(\"Image folder exists:\", os.path.exists(IMAGE_FOLDER))\n",
    "\n",
    "# Check a few files in the directory\n",
    "sample_files = os.listdir(IMAGE_FOLDER)[:5]\n",
    "print(\"Sample files in the directory:\", sample_files)\n",
    "\n",
    "# Check if the first image exists\n",
    "image_path = os.path.join(IMAGE_FOLDER, f\"{train_annotations_df['number'].iloc[0]}.jpg\")\n",
    "print(\"First image exists:\", os.path.exists(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data:\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "Label (Class): tensor(0, dtype=torch.int8)\n",
      "Valence: tensor(0.0306, dtype=torch.float16)\n",
      "Arousal: tensor(-0.0077, dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set parameters\n",
    "BATCHSIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LR = 0.0001#4e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# **** Create dataset and data loaders ****\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None, balance=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.balance = balance\n",
    "\n",
    "        if self.balance:\n",
    "            self.dataframe = self.balance_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(\n",
    "            self.root_dir, f\"{self.dataframe['number'].iloc[idx]}.jpg\"\n",
    "        )\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        classes = torch.tensor(self.dataframe.iloc[idx, 1], dtype=torch.int8)\n",
    "        valence = torch.tensor(self.dataframe.iloc[idx, 2], dtype=torch.float16)\n",
    "        arousal = torch.tensor(self.dataframe.iloc[idx, 3], dtype=torch.float16)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, classes, valence, arousal\n",
    "\n",
    "    def balance_dataset(self):\n",
    "        balanced_df = self.dataframe.groupby(\"exp\", group_keys=False).apply(\n",
    "            lambda x: x.sample(self.dataframe[\"exp\"].value_counts().min())\n",
    "        )\n",
    "        return balanced_df\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomGrayscale(0.01),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "        ),  # model more robust to changes in lighting conditions.\n",
    "        transforms.RandomPerspective(\n",
    "            distortion_scale=0.2, p=0.5\n",
    "        ),  # can be helpful if your images might have varying perspectives.\n",
    "        transforms.ToTensor(),  # saves image as tensor (automatically divides by 255)\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(\n",
    "            p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=\"random\"\n",
    "        ),  # Should help overfitting\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_valid = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Select the first 1000 indices for faster experimentation\n",
    "subset_indices = list(range(100000))\n",
    "\n",
    "# Create a subset of the training dataset\n",
    "train_subset = Subset(\n",
    "    CustomDataset(\n",
    "        dataframe=train_annotations_df,\n",
    "        root_dir=IMAGE_FOLDER,\n",
    "        transform=transform,\n",
    "        balance=True,\n",
    "    ),\n",
    "    subset_indices,\n",
    ")\n",
    "\n",
    "# val_subset = Subset(\n",
    "#     CustomDataset(\n",
    "#         dataframe=train_annotations_df,\n",
    "#         root_dir=IMAGE_FOLDER_TEST,\n",
    "#         transform=transform,\n",
    "#         balance=False,\n",
    "#     ),\n",
    "#     subset_indices,\n",
    "# )\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    dataframe=train_annotations_df,\n",
    "    root_dir=IMAGE_FOLDER,\n",
    "    transform=transform,\n",
    "    balance=True,\n",
    ")\n",
    "\n",
    "\n",
    "valid_dataset = CustomDataset(\n",
    "    dataframe=valid_annotations_df,\n",
    "    root_dir=IMAGE_FOLDER_TEST,\n",
    "    transform=transform_valid,\n",
    "    balance=False,\n",
    ")\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=0\n",
    "# )\n",
    "# Use the subset in the DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=BATCHSIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=BATCHSIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "sample = train_subset[0]  # Access the first sample\n",
    "image, label, valence, arousal = sample\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "print(\"Image shape:\", image.shape if isinstance(image, torch.Tensor) else \"Not Tensor\")\n",
    "print(\"Label (Class):\", label)\n",
    "print(\"Valence:\", valence)\n",
    "print(\"Arousal:\", arousal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of Images:\n",
      "torch.Size([32, 3, 224, 224])\n",
      "Batch of Classes (Labels):\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 2, 1, 1, 0, 2, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 2], dtype=torch.int8)\n",
      "Batch of Valence Values:\n",
      "tensor([ 0.7769,  0.8228, -0.0079,  0.8711, -0.2266,  0.0000,  0.2408,  0.0079,\n",
      "        -0.3730,  0.8018, -0.0079, -0.0635, -0.9126,  0.8857,  0.3174, -0.2581,\n",
      "        -0.8237, -0.4226, -0.3579,  0.5078,  0.5317,  0.6982,  0.0159, -0.4456,\n",
      "        -0.1404,  0.8569,  0.5952,  0.7461, -0.4062,  0.5181,  0.4048, -0.1984],\n",
      "       dtype=torch.float16)\n",
      "Batch of Arousal Values:\n",
      "tensor([ 0.3884,  0.2412,  0.0000, -0.0048,  0.0843,  0.0000, -0.4060,  0.0159,\n",
      "        -0.1031,  0.3491,  0.0079, -0.0793, -0.3254, -0.0775,  0.1349,  0.1159,\n",
      "        -0.4097,  0.2255,  0.2064,  0.0555,  0.0079,  0.6348, -0.0159,  0.2019,\n",
      "        -0.3533,  0.2460,  0.0238, -0.1270,  0.1711,  0.1749,  0.0317, -0.3889],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Display a single batch from the train_loader\n",
    "for images, classes, valence, arousal in train_loader:\n",
    "    print(\"Batch of Images:\")\n",
    "    print(images.shape)  # Shape of the images tensor\n",
    "    print(\"Batch of Classes (Labels):\")\n",
    "    print(classes)  # Tensor of class labels\n",
    "    print(\"Batch of Valence Values:\")\n",
    "    print(valence)  # Tensor of valence values\n",
    "    print(\"Batch of Arousal Values:\")\n",
    "    print(arousal)  # Tensor of arousal values\n",
    "    break  # Break after the first batch to avoid printing all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    134415\n",
      "0     74874\n",
      "2     25459\n",
      "6     24882\n",
      "3     14090\n",
      "4      6378\n",
      "5      3803\n",
      "7      3750\n",
      "Name: exp, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_annotations_df['exp'].value_counts())  # Check class distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 313/313 [03:10<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 313/313 [01:17<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.2497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 313/313 [01:44<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.2266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 313/313 [01:27<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.2137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 313/313 [01:17<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 0.2031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 313/313 [01:17<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Training Loss: 0.1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 313/313 [01:17<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Training Loss: 0.1827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 313/313 [01:17<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Training Loss: 0.1796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 313/313 [01:17<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Training Loss: 0.1742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 313/313 [01:17<00:00,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training Loss: 0.1688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch  # type: ignore\n",
    "import torch.nn as nn  # type: ignore\n",
    "import torch.nn.functional as F  # type: ignore\n",
    "import torch.optim as optim  # type: ignore\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the Residual Network\n",
    "class ResidualNetwork(nn.Module):\n",
    "    def __init__(self, num_outputs=2):  # 2 outputs for valence and arousal\n",
    "        super(ResidualNetwork, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_outputs)  # 2 outputs for valence and arousal\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        layers = [ResidualBlock(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = ResidualNetwork(num_outputs=2).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# Initialize gradient scaler for mixed precision\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, _, valence, arousal in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        # Move data to the device\n",
    "        images = images.to(DEVICE)\n",
    "        valence = valence.to(DEVICE, dtype=torch.float32)\n",
    "        arousal = arousal.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision forward pass\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(images)\n",
    "            val_pred, aro_pred = outputs[:, 0], outputs[:, 1]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(val_pred, valence) + criterion(aro_pred, arousal)\n",
    "\n",
    "        # Backward pass with scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Validation: 100%|██████████| 125/125 [00:05<00:00, 22.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Validation Loss: 0.5205\n"
     ]
    }
   ],
   "source": [
    "# Validation Phase\n",
    "model.eval()\n",
    "valid_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _, valence, arousal in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Validation\"):\n",
    "        # Move data to the device\n",
    "        images = images.to(DEVICE)\n",
    "        valence = valence.to(DEVICE, dtype=torch.float32)\n",
    "        arousal = arousal.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "        # Mixed precision forward pass\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(images)\n",
    "            val_pred, aro_pred = outputs[:, 0], outputs[:, 1]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(val_pred, valence) + criterion(aro_pred, arousal)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "valid_loss /= len(valid_loader)\n",
    "print(f\"Epoch {epoch+1}, Validation Loss: {valid_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
